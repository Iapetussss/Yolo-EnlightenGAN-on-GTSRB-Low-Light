"""
è¯Šæ–­è®­ç»ƒç»“æœ - æ£€æŸ¥ 98.65% mAP æ˜¯å¦åˆç†
"""

from pathlib import Path
import pandas as pd
import yaml

def check_dataset_split():
    """æ£€æŸ¥æ•°æ®é›†åˆ’åˆ†æ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 70)
    print("1. æ£€æŸ¥æ•°æ®é›†åˆ’åˆ†")
    print("=" * 70)
    
    # è¯»å– YAML é…ç½®
    yaml_path = Path('traffic_signs_dataset.yaml')
    with open(yaml_path, 'r') as f:
        config = yaml.safe_load(f)
    
    dataset_path = Path(config['path'])
    
    # ç»Ÿè®¡å„é›†å›¾åƒæ•°
    splits = {}
    for split in ['train', 'val', 'test']:
        img_dir = dataset_path / 'images' / split
        label_dir = dataset_path / 'labels' / split
        
        if img_dir.exists():
            images = list(img_dir.glob('*.png')) + list(img_dir.glob('*.jpg'))
            labels = list(label_dir.glob('*.txt'))
            splits[split] = {
                'images': len(images),
                'labels': len(labels)
            }
        else:
            splits[split] = {'images': 0, 'labels': 0}
    
    print(f"\næ•°æ®é›†è·¯å¾„: {dataset_path}")
    print("\næ•°æ®ç»Ÿè®¡:")
    print(f"{'é›†åˆ':<10} {'å›¾åƒæ•°':<10} {'æ ‡ç­¾æ•°':<10} {'åŒ¹é…':<10}")
    print("-" * 50)
    
    for split, counts in splits.items():
        match = "âœ…" if counts['images'] == counts['labels'] else "âŒ"
        print(f"{split:<10} {counts['images']:<10} {counts['labels']:<10} {match:<10}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å 
    print("\næ£€æŸ¥æ•°æ®æ³„éœ²...")
    train_imgs = set([f.name for f in (dataset_path / 'images' / 'train').glob('*')])
    val_imgs = set([f.name for f in (dataset_path / 'images' / 'val').glob('*')])
    
    overlap = train_imgs & val_imgs
    if overlap:
        print(f"âŒ è­¦å‘Šï¼šè®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰ {len(overlap)} ä¸ªé‡å¤å›¾åƒï¼")
        print(f"   è¿™å¯èƒ½å¯¼è‡´è™šé«˜çš„ mAPï¼")
        print(f"   ç¤ºä¾‹ï¼š{list(overlap)[:5]}")
    else:
        print("âœ… è®­ç»ƒé›†å’ŒéªŒè¯é›†æ— é‡å ")
    
    return splits, len(overlap) > 0

def check_training_results():
    """æ£€æŸ¥è®­ç»ƒç»“æœ"""
    print("\n" + "=" * 70)
    print("2. åˆ†æè®­ç»ƒç»“æœ")
    print("=" * 70)
    
    results_csv = Path('runs/train/gtsrb_enlightengan8/results.csv')
    
    if not results_csv.exists():
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒç»“æœæ–‡ä»¶")
        return None
    
    df = pd.read_csv(results_csv)
    df.columns = df.columns.str.strip()
    
    # æœ€ç»ˆç»“æœ
    final_row = df.iloc[-1]
    
    print("\næœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:")
    print(f"  mAP@0.5:      {final_row['metrics/mAP50(B)']:.4f} ({final_row['metrics/mAP50(B)']*100:.2f}%)")
    print(f"  mAP@0.5:0.95: {final_row['metrics/mAP50-95(B)']:.4f} ({final_row['metrics/mAP50-95(B)']*100:.2f}%)")
    print(f"  Precision:    {final_row['metrics/precision(B)']:.4f} ({final_row['metrics/precision(B)']*100:.2f}%)")
    print(f"  Recall:       {final_row['metrics/recall(B)']:.4f} ({final_row['metrics/recall(B)']*100:.2f}%)")
    
    # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
    print("\nè®­ç»ƒ vs éªŒè¯æŸå¤±:")
    try:
        train_box_loss = final_row['train/box_loss']
        val_box_loss = final_row['val/box_loss']
        
        print(f"  è®­ç»ƒé›† Box Loss: {train_box_loss:.4f}")
        print(f"  éªŒè¯é›† Box Loss: {val_box_loss:.4f}")
        
        if val_box_loss < train_box_loss * 0.8:
            print("  âš ï¸  éªŒè¯æŸå¤±è¿œä½äºè®­ç»ƒæŸå¤±ï¼Œå¯èƒ½æœ‰é—®é¢˜ï¼")
        elif val_box_loss > train_box_loss * 1.5:
            print("  âš ï¸  è¿‡æ‹Ÿåˆï¼šéªŒè¯æŸå¤±è¿œé«˜äºè®­ç»ƒæŸå¤±")
        else:
            print("  âœ… æŸå¤±æ¯”ä¾‹æ­£å¸¸")
    except:
        print("  âš ï¸  æ— æ³•æ¯”è¾ƒæŸå¤±")
    
    # æ£€æŸ¥æ”¶æ•›
    print("\nè®­ç»ƒæ”¶æ•›åˆ†æ:")
    last_5 = df.tail(5)
    map_variance = last_5['metrics/mAP50(B)'].std()
    print(f"  æœ€å5è½® mAP æ ‡å‡†å·®: {map_variance:.6f}")
    
    if map_variance < 0.001:
        print("  âœ… å·²å……åˆ†æ”¶æ•›")
    elif map_variance < 0.01:
        print("  âœ… åŸºæœ¬æ”¶æ•›")
    else:
        print("  âš ï¸  å¯èƒ½æœªå®Œå…¨æ”¶æ•›ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
    
    return df

def check_task_difficulty():
    """è¯„ä¼°ä»»åŠ¡éš¾åº¦"""
    print("\n" + "=" * 70)
    print("3. ä»»åŠ¡éš¾åº¦è¯„ä¼°")
    print("=" * 70)
    
    print("\nGTSRB äº¤é€šæ ‡å¿—æ£€æµ‹çš„å…¸å‹éš¾åº¦:")
    print("  â€¢ ç±»åˆ«æ•°: 43 ç±»")
    print("  â€¢ å›¾åƒè´¨é‡: é«˜ï¼ˆçœŸå®æ‹æ‘„ï¼‰")
    print("  â€¢ ç›®æ ‡å¤§å°: ä¸­ç­‰ï¼ˆäº¤é€šæ ‡å¿—é€šå¸¸å æ¯”è¾ƒå¤§ï¼‰")
    print("  â€¢ èƒŒæ™¯å¤æ‚åº¦: ä¸­ç­‰")
    print("  â€¢ é®æŒ¡æƒ…å†µ: è¾ƒå°‘")
    
    print("\né¢†åŸŸåŸºå‡†ï¼ˆGTSRB æ•°æ®é›†ï¼‰:")
    print("  â€¢ æ­£å¸¸å…‰ç…§ + YOLOv8n: 88-95% mAP@0.5")
    print("  â€¢ ä½å…‰ç…§ + YOLOv8nï¼ˆæ— å¢å¼ºï¼‰: 55-70% mAP@0.5")
    print("  â€¢ ä½å…‰ç…§ + ä¼ ç»Ÿå¢å¼º: 75-88% mAP@0.5")
    print("  â€¢ ä½å…‰ç…§ + æ·±åº¦å­¦ä¹ å¢å¼º: 80-92% mAP@0.5")
    
    print("\nä½ çš„ç»“æœ: 98.65% mAP@0.5")
    print("  â†’ è¿œé«˜äºå…¸å‹è¡¨ç°ï¼")
    
    print("\nå¯èƒ½çš„åŸå› :")
    print("  1. âœ… æ•°æ®å¢å¼ºæ•ˆæœç‰¹åˆ«å¥½")
    print("  2. âœ… è®­ç»ƒç­–ç•¥ä¼˜ç§€")
    print("  3. âš ï¸  æ•°æ®æ³„éœ²ï¼ˆè®­ç»ƒé›†å’ŒéªŒè¯é›†é‡å ï¼‰")
    print("  4. âš ï¸  éªŒè¯é›†å¤ªç®€å•")
    print("  5. âš ï¸  è¯„ä¼°æ–¹å¼æœ‰è¯¯")

def check_validation_cache():
    """æ£€æŸ¥éªŒè¯ç¼“å­˜"""
    print("\n" + "=" * 70)
    print("4. æ£€æŸ¥æ•°æ®ç¼“å­˜")
    print("=" * 70)
    
    cache_files = [
        'yolo_dataset/images/train.cache',
        'yolo_dataset/images/val.cache',
        'yolo_dataset/labels/train.cache',
        'yolo_dataset/labels/val.cache',
    ]
    
    print("\nç¼“å­˜æ–‡ä»¶:")
    for cache in cache_files:
        cache_path = Path(cache)
        if cache_path.exists():
            size_mb = cache_path.stat().st_size / (1024 * 1024)
            print(f"  âœ… {cache} ({size_mb:.2f} MB)")
        else:
            print(f"  âŒ {cache} (ä¸å­˜åœ¨)")
    
    print("\nğŸ’¡ å»ºè®®ï¼šåˆ é™¤ç¼“å­˜é‡æ–°éªŒè¯")
    print("   rm yolo_dataset/images/*.cache")
    print("   python step7_evaluate_model.py")

def detailed_class_analysis():
    """è¯¦ç»†çš„ç±»åˆ«åˆ†æ"""
    print("\n" + "=" * 70)
    print("5. ç±»åˆ«çº§åˆ«åˆ†æ")
    print("=" * 70)
    
    results_dir = Path('runs/train/gtsrb_enlightengan8')
    
    # æ£€æŸ¥æ··æ·†çŸ©é˜µ
    confusion_matrix = results_dir / 'confusion_matrix.png'
    if confusion_matrix.exists():
        print(f"\nâœ… æ··æ·†çŸ©é˜µ: {confusion_matrix}")
        print("   æ‰“å¼€æŸ¥çœ‹æ˜¯å¦å¯¹è§’çº¿è¿‡äºå®Œç¾")
    
    # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
    print("\nå¦‚æœæ‰€æœ‰ç±»åˆ«éƒ½æ¥è¿‘ 100%ï¼Œå¯èƒ½æœ‰é—®é¢˜")
    print("æ­£å¸¸æƒ…å†µä¸‹ï¼š")
    print("  â€¢ ç®€å•ç±»åˆ«ï¼ˆå¦‚ STOPï¼‰: 95-99%")
    print("  â€¢ ä¸­ç­‰ç±»åˆ«: 85-95%")
    print("  â€¢ å›°éš¾ç±»åˆ«ï¼ˆå°æ ·æœ¬ï¼‰: 70-85%")

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("  è®­ç»ƒç»“æœè¯Šæ–­å·¥å…·".center(70))
    print("  æ£€æŸ¥ 98.65% mAP æ˜¯å¦åˆç†".center(70))
    print("=" * 70)
    
    # 1. æ£€æŸ¥æ•°æ®é›†
    splits, has_overlap = check_dataset_split()
    
    # 2. åˆ†æè®­ç»ƒç»“æœ
    df = check_training_results()
    
    # 3. ä»»åŠ¡éš¾åº¦
    check_task_difficulty()
    
    # 4. ç¼“å­˜æ£€æŸ¥
    check_validation_cache()
    
    # 5. ç±»åˆ«åˆ†æ
    detailed_class_analysis()
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("  è¯Šæ–­æ€»ç»“".center(70))
    print("=" * 70)
    
    issues = []
    
    if has_overlap:
        issues.append("âŒ ä¸¥é‡ï¼šè®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰é‡å ")
    
    if df is not None:
        final_map = df.iloc[-1]['metrics/mAP50(B)']
        if final_map > 0.97:
            issues.append("âš ï¸  è­¦å‘Šï¼šmAP å¼‚å¸¸é«˜ï¼ˆ>97%ï¼‰")
    
    if not issues:
        print("\nâœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
        print("\nå¯èƒ½çš„åŸå› :")
        print("  1. GTSRB æœ¬èº«æ˜¯ç›¸å¯¹ç®€å•çš„æ•°æ®é›†")
        print("  2. å›¾åƒå¢å¼ºæ•ˆæœç¡®å®å¾ˆå¥½")
        print("  3. YOLOv8n åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€")
        print("\nå»ºè®®:")
        print("  â€¢ åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆæ›´å‡†ç¡®ï¼‰")
        print("  â€¢ ä¸å…¶ä»–è®ºæ–‡å¯¹æ¯”")
        print("  â€¢ å¯è§†åŒ–æ£€æŸ¥é¢„æµ‹ç»“æœ")
    else:
        print("\nâš ï¸  å‘ç°æ½œåœ¨é—®é¢˜:")
        for issue in issues:
            print(f"  {issue}")
        
        print("\nå»ºè®®æªæ–½:")
        print("  1. æ£€æŸ¥æ•°æ®åˆ’åˆ†è„šæœ¬")
        print("  2. åˆ é™¤ç¼“å­˜é‡æ–°è¯„ä¼°")
        print("  3. åœ¨çœŸæ­£çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°")
        print("  4. å¯è§†åŒ–é¢„æµ‹ç»“æœ")
        print("  5. æŸ¥çœ‹æ··æ·†çŸ©é˜µ")
    
    print("\n" + "=" * 70)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“„ ä¸‹ä¸€æ­¥ï¼š")
    print("  1. æŸ¥çœ‹æ··æ·†çŸ©é˜µ:")
    print("     runs/train/gtsrb_enlightengan8/confusion_matrix.png")
    print("\n  2. é‡æ–°è¯„ä¼°ï¼ˆåˆ é™¤ç¼“å­˜ï¼‰:")
    print("     python step7_evaluate_model.py")
    print("\n  3. æµ‹è¯•å•å¼ å›¾åƒ:")
    print("     python step8_test_single_image.py")

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

